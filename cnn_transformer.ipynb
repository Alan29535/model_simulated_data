{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4qOnzlqJzuR",
        "outputId": "c1d5689b-18da-48c7-a35a-e07995778507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "Le0bbi03J9yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4OuEAOCJ--6",
        "outputId": "5ad41533-c937-492f-853b-9613250d702f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U fastai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgLeQHS7KBkr",
        "outputId": "176a06ef-b82d-4a47-b288-6eeb6ee15a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastai in /usr/local/lib/python3.7/dist-packages (2.7.10)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai) (3.4.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (6.0)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.0.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (21.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (21.1.3)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.3)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.7.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.4.5 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.5.27)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.13.1+cu113)\n",
            "Requirement already satisfied: torch<1.14,>=1.7 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.12.1+cu113)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.4.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (8.1.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.10.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (4.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastai) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4->fastai) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4->fastai) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2022.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01RgRYZ9KFfB",
        "outputId": "f4e4c03c-72ac-4ed8-d6b3-b0f50bc9a0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.43)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 63.0 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=0798c6571367f1ab1b1deb8b6bfa7b912dcb628b4e111e800c4036755d21791b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.11.0 pyperclip-1.8.2 stevedore-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tcn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_j79BokMCvv",
        "outputId": "94538ddd-368b-4671-f380-fd70d065be62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tcn\n",
            "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tcn) (1.21.6)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-tcn) (2.9.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (57.4.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (1.12)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (2.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (3.1.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (14.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (1.50.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (2.9.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (1.14.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (3.19.6)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (4.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-tcn) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-tcn) (0.38.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-tcn) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2.14.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-tcn) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow->keras-tcn) (3.0.9)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->keras-tcn) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons, keras-tcn\n",
            "Successfully installed keras-tcn-3.5.0 tensorflow-addons-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(1)\n",
        "import random\n",
        "from tensorflow import keras\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from tensorflow.keras.layers import BatchNormalization,Dropout\n",
        "from tensorflow.keras import models, layers, losses, optimizers, activations, regularizers\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "fy3jbUnCKH7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "dtSonAMLKJgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.visualization import plot_contour\n",
        "from optuna.visualization import plot_edf\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_parallel_coordinate\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_slice\n",
        "import joblib\n",
        "import pickle\n",
        "from keras.engine.input_spec import InputSpec\n",
        "initializer = tf.keras.initializers.Zeros()\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
        "from tcn import TCN"
      ],
      "metadata": {
        "id": "b1BTJLikKLAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flow = np.load('/content/drive/MyDrive/CNN model data/lung_data/flow400.npy')\n",
        "volume = np.load('/content/drive/MyDrive/CNN model data/lung_data/volume400.npy')\n",
        "paw = np.load('/content/drive/MyDrive/CNN model data/lung_data/paw400.npy')\n",
        "capacitances = np.load('/content/drive/MyDrive/CNN model data/lung_data/capacitances400.npy')\n",
        "resistances = np.load(\"/content/drive/MyDrive/CNN model data/lung_data/rins400.npy\")\n",
        "#output1 = np.load(\"/content/drive/MyDrive/CNN model data/output1.npy\")\n",
        "#input1 = np.load(\"/content/drive/MyDrive/CNN model data/input1.npy\")"
      ],
      "metadata": {
        "id": "Hr85gKJ2KMkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_random_seeds():\n",
        "   os.environ['PYTHONHASHSEED']=str(1)\n",
        "   tf.random.set_seed(1)\n",
        "   np.random.seed(1)\n",
        "   random.seed(1)"
      ],
      "metadata": {
        "id": "1sMe_rGGKN2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(data, minimum = None,maximum = None):\n",
        "    if minimum is None:\n",
        "        minimum = np.min(np.min(data))\n",
        "    if maximum is None:\n",
        "        maximum = np.max(np.max(data))\n",
        "    data_norm = (data - minimum) / (maximum - minimum)\n",
        "    return minimum, maximum, data_norm\n"
      ],
      "metadata": {
        "id": "BxCOP5ysKPGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flow = flow.T\n",
        "volume = volume.T\n",
        "paw = paw.T\n",
        "resistances = resistances.T\n",
        "capacitances = capacitances.T\n",
        "\n",
        "print(\"transposed\")\n",
        "\n",
        "num_examples = flow.shape[0]\n",
        "num_samples = flow.shape[1]\n",
        "print(\"num_samples are\",num_examples)\n",
        "(min_flow, max_flow, flow) = normalize_data(flow)\n",
        "(min_volume, max_volume, volume) = normalize_data(volume)\n",
        "(min_paw, max_paw, paw) = normalize_data(paw)\n",
        "(min_resistance, max_resistance, resistances) = normalize_data(resistances)\n",
        "(min_capacitance, max_capacitance, capacitances) = normalize_data(capacitances)\n",
        "\n",
        "print(\"normalized data\")\n",
        "\n",
        "input_data = np.zeros((num_examples, num_samples, 3))\n",
        "input_data[:, :, 0] = flow\n",
        "input_data[:, :, 1] = volume\n",
        "input_data[:, :, 2] = paw\n",
        "output_data = np.concatenate((resistances, capacitances), axis=1)\n",
        "indices = np.arange(num_examples)\n",
        "\n",
        "print(\"input created\")\n",
        "\n",
        "input_train, input_test, output_train, output_test, indices_train, indices_test = \\\n",
        "    train_test_split(input_data, output_data, indices, test_size=0.3, shuffle=False, random_state=11)\n",
        "\n",
        "input_validation, input_test, output_validation, output_test, indices_validation, indices_test = \\\n",
        "    train_test_split(input_test, output_test, indices_test, test_size=0.5, shuffle=False, random_state=11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TV5QqTBKQXR",
        "outputId": "c9187fa6-09f6-447a-e576-dc4efbc1cfb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transposed\n",
            "num_samples are 400\n",
            "normalized data\n",
            "input created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "\n",
        "\n",
        "    \n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return res + x"
      ],
      "metadata": {
        "id": "kMd-I45Zbxw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    x = inputs\n",
        "    x = layers.Conv1D(filters=2, kernel_size=2)(inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.BatchNormalization(name='norm_6')(x)\n",
        "    x = layers.LeakyReLU(alpha=0.1, name='leaky_relu_6')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "\n",
        "    x = layers.Dense(200, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.01)(x)\n",
        "    x = layers.Dense(50, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.01)(x)\n",
        "    outputs = layers.Dense(2)(x)\n",
        "    return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "vPg6_NkjbySh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = input_train.shape[1:]"
      ],
      "metadata": {
        "id": "H8jGHWicb1mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    reset_random_seeds()\n",
        "    params = {\n",
        "              'learning_rate': trial.suggest_uniform('learning_rate', 1e-5, 1e-3),\n",
        "              'head_size': trial.suggest_int(\"head_size\",128,256),\n",
        "              'num_heads': trial.suggest_int(\"num_heads\",3,4,5),\n",
        "              'ff_dim': trial.suggest_int(\"ff_dim\",2,3,4),\n",
        "              'num_transformer_bock': trial.suggest_int(\"num_transformer_bock\",3,4),\n",
        "              'decay': trial.suggest_categorical(\"decay\",[1e-3, 1e-5]),\n",
        "              'dropout_rate': trial.suggest_categorical(\"dropout_rate\", [0.0,0.3]),\n",
        "              'epochs': trial.suggest_categorical(\"epochs\", [100,150])\n",
        "              }\n",
        "    model = build_model(\n",
        "    input_shape,\n",
        "    head_size=params['head_size'],\n",
        "    num_heads=params['num_heads'],\n",
        "    ff_dim=params['ff_dim'],\n",
        "    num_transformer_blocks=params['num_transformer_bock'],\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.0,\n",
        "    dropout=0.0,\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=optimizers.Adam(lr=params['learning_rate'], decay=params['decay']), loss=losses.mean_squared_error)\n",
        "\n",
        "    model.fit(input_train, output_train, epochs=params['epochs'], verbose=1,\n",
        "            validation_data=(input_validation, output_validation)\n",
        "            )\n",
        "\n",
        "    y_pred = model.predict(input_test)\n",
        "    mse = mean_squared_error(output_test, y_pred)\n",
        "    mae = mean_absolute_error(output_test,y_pred)\n",
        "    mape = mean_absolute_percentage_error(output_test,y_pred)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/tcn+transformer/{}.pickle\".format(trial.number), \"wb\") as fout:\n",
        "        pickle.dump(model, fout)\n",
        "    return mse"
      ],
      "metadata": {
        "id": "DWYskzGrb3F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "reset_random_seeds()\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=21))\n",
        "study.optimize(objective, n_trials=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jnhCR1gb6Xn",
        "outputId": "5dcb93e1-2724-4ff8-a8cf-3ed375de3460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-11-17 17:02:23,804]\u001b[0m A new study created in memory with name: no-name-fabb2ab4-e875-4ac9-986c-8428c20b471a\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 21s 2s/step - loss: 0.6176 - val_loss: 0.1733\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.3656 - val_loss: 0.2502\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.2908 - val_loss: 0.1074\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.2398 - val_loss: 0.1038\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.2191 - val_loss: 0.1207\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.1834 - val_loss: 0.0897\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.1697 - val_loss: 0.1054\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.1533 - val_loss: 0.0882\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.1504 - val_loss: 0.0956\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.1306 - val_loss: 0.0924\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.1155 - val_loss: 0.0958\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.1160 - val_loss: 0.1050\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.1157 - val_loss: 0.0992\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.1069 - val_loss: 0.1007\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.1062 - val_loss: 0.1007\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0950 - val_loss: 0.1005\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0939 - val_loss: 0.0987\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0924 - val_loss: 0.1061\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0876 - val_loss: 0.1051\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0846 - val_loss: 0.1061\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0758 - val_loss: 0.1033\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0758 - val_loss: 0.0969\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0742 - val_loss: 0.0990\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0714 - val_loss: 0.0962\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0720 - val_loss: 0.0955\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0718 - val_loss: 0.0878\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0681 - val_loss: 0.0906\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0712 - val_loss: 0.0914\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0666 - val_loss: 0.0851\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0665 - val_loss: 0.0915\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0648 - val_loss: 0.0876\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0627 - val_loss: 0.0885\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0603 - val_loss: 0.0852\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0576 - val_loss: 0.0857\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0694 - val_loss: 0.0854\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0563 - val_loss: 0.0807\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 19s 2s/step - loss: 0.0577 - val_loss: 0.0903\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0573 - val_loss: 0.0787\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0661 - val_loss: 0.0791\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0545 - val_loss: 0.0790\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0544 - val_loss: 0.0813\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0514 - val_loss: 0.0767\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0523 - val_loss: 0.0771\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0532 - val_loss: 0.0790\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0484 - val_loss: 0.0758\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0538 - val_loss: 0.0761\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0483 - val_loss: 0.0802\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0524 - val_loss: 0.0688\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0462 - val_loss: 0.0815\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0487 - val_loss: 0.0725\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0502 - val_loss: 0.0746\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0439 - val_loss: 0.0766\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0449 - val_loss: 0.0660\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0512 - val_loss: 0.0718\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0427 - val_loss: 0.0711\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0435 - val_loss: 0.0689\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0425 - val_loss: 0.0714\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0431 - val_loss: 0.0672\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0442 - val_loss: 0.0650\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0444 - val_loss: 0.0692\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0427 - val_loss: 0.0616\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0432 - val_loss: 0.0641\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0436 - val_loss: 0.0657\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0394 - val_loss: 0.0565\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0377 - val_loss: 0.0650\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0380 - val_loss: 0.0578\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0372 - val_loss: 0.0595\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0381 - val_loss: 0.0593\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0372 - val_loss: 0.0562\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0390 - val_loss: 0.0573\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0341 - val_loss: 0.0574\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0385 - val_loss: 0.0574\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0378 - val_loss: 0.0550\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0345 - val_loss: 0.0585\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0338 - val_loss: 0.0561\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0356 - val_loss: 0.0553\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0345 - val_loss: 0.0588\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0354 - val_loss: 0.0532\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0347 - val_loss: 0.0595\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0364 - val_loss: 0.0530\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0341 - val_loss: 0.0576\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0321 - val_loss: 0.0528\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0320 - val_loss: 0.0579\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0349 - val_loss: 0.0531\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0314 - val_loss: 0.0564\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0352 - val_loss: 0.0534\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0343 - val_loss: 0.0532\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0329 - val_loss: 0.0542\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0327 - val_loss: 0.0536\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0321 - val_loss: 0.0550\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0332 - val_loss: 0.0526\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0317 - val_loss: 0.0562\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0311 - val_loss: 0.0517\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 18s 2s/step - loss: 0.0333 - val_loss: 0.0546\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0267 - val_loss: 0.0519\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0326 - val_loss: 0.0543\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0290 - val_loss: 0.0516\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0297 - val_loss: 0.0553\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0296 - val_loss: 0.0534\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 17s 2s/step - loss: 0.0288 - val_loss: 0.0536\n",
            "2/2 [==============================] - 2s 725ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n",
            "\u001b[32m[I 2022-11-17 17:31:46,556]\u001b[0m Trial 0 finished with value: 0.04525207126366099 and parameters: {'learning_rate': 5.8237632001036017e-05, 'head_size': 165, 'num_heads': 3, 'ff_dim': 2, 'num_transformer_bock': 4, 'decay': 1e-05, 'dropout_rate': 0.3, 'epochs': 100}. Best is trial 0 with value: 0.04525207126366099.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:686: UserWarning: The distribution is specified by [3, 4] and step=5, but the range is not divisible by `step`. It will be replaced by [3, 3].\n",
            "  low=low, old_high=old_high, high=high, step=step\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:686: UserWarning: The distribution is specified by [2, 3] and step=4, but the range is not divisible by `step`. It will be replaced by [2, 2].\n",
            "  low=low, old_high=old_high, high=high, step=step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 20s 2s/step - loss: 1.7994 - val_loss: 0.2640\n",
            "Epoch 2/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.2098 - val_loss: 0.1578\n",
            "Epoch 3/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.1529 - val_loss: 0.1607\n",
            "Epoch 4/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0969 - val_loss: 0.1357\n",
            "Epoch 5/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0746 - val_loss: 0.1209\n",
            "Epoch 6/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0602 - val_loss: 0.1229\n",
            "Epoch 7/150\n",
            "9/9 [==============================] - 15s 2s/step - loss: 0.0501 - val_loss: 0.1189\n",
            "Epoch 8/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0433 - val_loss: 0.1117\n",
            "Epoch 9/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0385 - val_loss: 0.1086\n",
            "Epoch 10/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0338 - val_loss: 0.1092\n",
            "Epoch 11/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0338 - val_loss: 0.1123\n",
            "Epoch 12/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0340 - val_loss: 0.1092\n",
            "Epoch 13/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0321 - val_loss: 0.1043\n",
            "Epoch 14/150\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0277 - val_loss: 0.1069\n",
            "Epoch 15/150\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0264 - val_loss: 0.1056\n",
            "Epoch 16/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0247 - val_loss: 0.1005\n",
            "Epoch 17/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0238 - val_loss: 0.1049\n",
            "Epoch 18/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0233 - val_loss: 0.1036\n",
            "Epoch 19/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0214 - val_loss: 0.0978\n",
            "Epoch 20/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0226 - val_loss: 0.0937\n",
            "Epoch 21/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0194 - val_loss: 0.0900\n",
            "Epoch 22/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0186 - val_loss: 0.0908\n",
            "Epoch 23/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0175 - val_loss: 0.0903\n",
            "Epoch 24/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0163 - val_loss: 0.0877\n",
            "Epoch 25/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0159 - val_loss: 0.0888\n",
            "Epoch 26/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0154 - val_loss: 0.0878\n",
            "Epoch 27/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0157 - val_loss: 0.0868\n",
            "Epoch 28/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0150 - val_loss: 0.0824\n",
            "Epoch 29/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0156 - val_loss: 0.0782\n",
            "Epoch 30/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0147 - val_loss: 0.0759\n",
            "Epoch 31/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0150 - val_loss: 0.0761\n",
            "Epoch 32/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0154 - val_loss: 0.0758\n",
            "Epoch 33/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0126 - val_loss: 0.0802\n",
            "Epoch 34/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0137 - val_loss: 0.0713\n",
            "Epoch 35/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0145 - val_loss: 0.0732\n",
            "Epoch 36/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0121 - val_loss: 0.0717\n",
            "Epoch 37/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0147 - val_loss: 0.0713\n",
            "Epoch 38/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0112 - val_loss: 0.0745\n",
            "Epoch 39/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0138 - val_loss: 0.0714\n",
            "Epoch 40/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0118 - val_loss: 0.0696\n",
            "Epoch 41/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0130 - val_loss: 0.0636\n",
            "Epoch 42/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0095 - val_loss: 0.0641\n",
            "Epoch 43/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0099 - val_loss: 0.0594\n",
            "Epoch 44/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0099 - val_loss: 0.0583\n",
            "Epoch 45/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0090 - val_loss: 0.0593\n",
            "Epoch 46/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0087 - val_loss: 0.0600\n",
            "Epoch 47/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0083 - val_loss: 0.0585\n",
            "Epoch 48/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0091 - val_loss: 0.0580\n",
            "Epoch 49/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0086 - val_loss: 0.0595\n",
            "Epoch 50/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0079 - val_loss: 0.0586\n",
            "Epoch 51/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0101 - val_loss: 0.0615\n",
            "Epoch 52/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0079 - val_loss: 0.0582\n",
            "Epoch 53/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0081 - val_loss: 0.0584\n",
            "Epoch 54/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0079 - val_loss: 0.0596\n",
            "Epoch 55/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0084 - val_loss: 0.0607\n",
            "Epoch 56/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0079 - val_loss: 0.0603\n",
            "Epoch 57/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0092 - val_loss: 0.0599\n",
            "Epoch 58/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0084 - val_loss: 0.0589\n",
            "Epoch 59/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0067 - val_loss: 0.0579\n",
            "Epoch 60/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0068 - val_loss: 0.0553\n",
            "Epoch 61/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0067 - val_loss: 0.0614\n",
            "Epoch 62/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0062 - val_loss: 0.0578\n",
            "Epoch 63/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0070 - val_loss: 0.0588\n",
            "Epoch 64/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0066 - val_loss: 0.0591\n",
            "Epoch 65/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0062 - val_loss: 0.0580\n",
            "Epoch 66/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0051 - val_loss: 0.0630\n",
            "Epoch 67/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0060 - val_loss: 0.0599\n",
            "Epoch 68/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0063 - val_loss: 0.0606\n",
            "Epoch 69/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0058 - val_loss: 0.0626\n",
            "Epoch 70/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0058 - val_loss: 0.0616\n",
            "Epoch 71/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0046 - val_loss: 0.0627\n",
            "Epoch 72/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0057 - val_loss: 0.0618\n",
            "Epoch 73/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0062 - val_loss: 0.0571\n",
            "Epoch 74/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0053 - val_loss: 0.0620\n",
            "Epoch 75/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0042 - val_loss: 0.0616\n",
            "Epoch 76/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0051 - val_loss: 0.0630\n",
            "Epoch 77/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0058 - val_loss: 0.0602\n",
            "Epoch 78/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0042 - val_loss: 0.0575\n",
            "Epoch 79/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0045 - val_loss: 0.0583\n",
            "Epoch 80/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0058 - val_loss: 0.0618\n",
            "Epoch 81/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0052 - val_loss: 0.0597\n",
            "Epoch 82/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0045 - val_loss: 0.0591\n",
            "Epoch 83/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0048 - val_loss: 0.0551\n",
            "Epoch 84/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0052 - val_loss: 0.0619\n",
            "Epoch 85/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0064 - val_loss: 0.0585\n",
            "Epoch 86/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0054 - val_loss: 0.0560\n",
            "Epoch 87/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0057 - val_loss: 0.0587\n",
            "Epoch 88/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0043 - val_loss: 0.0605\n",
            "Epoch 89/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0053 - val_loss: 0.0611\n",
            "Epoch 90/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0037 - val_loss: 0.0589\n",
            "Epoch 91/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0042 - val_loss: 0.0578\n",
            "Epoch 92/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0035 - val_loss: 0.0565\n",
            "Epoch 93/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0051 - val_loss: 0.0572\n",
            "Epoch 94/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0044 - val_loss: 0.0579\n",
            "Epoch 95/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0041 - val_loss: 0.0587\n",
            "Epoch 96/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0037 - val_loss: 0.0593\n",
            "Epoch 97/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0045 - val_loss: 0.0572\n",
            "Epoch 98/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0047 - val_loss: 0.0590\n",
            "Epoch 99/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0037 - val_loss: 0.0572\n",
            "Epoch 100/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0034 - val_loss: 0.0588\n",
            "Epoch 101/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0034 - val_loss: 0.0565\n",
            "Epoch 102/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0027 - val_loss: 0.0574\n",
            "Epoch 103/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0035 - val_loss: 0.0569\n",
            "Epoch 104/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0051 - val_loss: 0.0543\n",
            "Epoch 105/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0041 - val_loss: 0.0532\n",
            "Epoch 106/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0045 - val_loss: 0.0577\n",
            "Epoch 107/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0037 - val_loss: 0.0573\n",
            "Epoch 108/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0036 - val_loss: 0.0540\n",
            "Epoch 109/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0041 - val_loss: 0.0547\n",
            "Epoch 110/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0054 - val_loss: 0.0541\n",
            "Epoch 111/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0039 - val_loss: 0.0594\n",
            "Epoch 112/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0052 - val_loss: 0.0558\n",
            "Epoch 113/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0043 - val_loss: 0.0548\n",
            "Epoch 114/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0041 - val_loss: 0.0541\n",
            "Epoch 115/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0041 - val_loss: 0.0544\n",
            "Epoch 116/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0045 - val_loss: 0.0539\n",
            "Epoch 117/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0042 - val_loss: 0.0535\n",
            "Epoch 118/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0035 - val_loss: 0.0578\n",
            "Epoch 119/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0033 - val_loss: 0.0558\n",
            "Epoch 120/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0036 - val_loss: 0.0525\n",
            "Epoch 121/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0037 - val_loss: 0.0556\n",
            "Epoch 122/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0035 - val_loss: 0.0544\n",
            "Epoch 123/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0032 - val_loss: 0.0528\n",
            "Epoch 124/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0035 - val_loss: 0.0553\n",
            "Epoch 125/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0036 - val_loss: 0.0555\n",
            "Epoch 126/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0031 - val_loss: 0.0549\n",
            "Epoch 127/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0034 - val_loss: 0.0539\n",
            "Epoch 128/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0038 - val_loss: 0.0546\n",
            "Epoch 129/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0034 - val_loss: 0.0542\n",
            "Epoch 130/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0033 - val_loss: 0.0544\n",
            "Epoch 131/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0039 - val_loss: 0.0554\n",
            "Epoch 132/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0034 - val_loss: 0.0543\n",
            "Epoch 133/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0035 - val_loss: 0.0560\n",
            "Epoch 134/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0030 - val_loss: 0.0566\n",
            "Epoch 135/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0029 - val_loss: 0.0568\n",
            "Epoch 136/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0047 - val_loss: 0.0565\n",
            "Epoch 137/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0026 - val_loss: 0.0537\n",
            "Epoch 138/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0048 - val_loss: 0.0548\n",
            "Epoch 139/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0028 - val_loss: 0.0560\n",
            "Epoch 140/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0033 - val_loss: 0.0559\n",
            "Epoch 141/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0039 - val_loss: 0.0554\n",
            "Epoch 142/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0032 - val_loss: 0.0547\n",
            "Epoch 143/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0032 - val_loss: 0.0564\n",
            "Epoch 144/150\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0028 - val_loss: 0.0570\n",
            "Epoch 145/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0026 - val_loss: 0.0571\n",
            "Epoch 146/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0031 - val_loss: 0.0569\n",
            "Epoch 147/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0026 - val_loss: 0.0585\n",
            "Epoch 148/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0020 - val_loss: 0.0579\n",
            "Epoch 149/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0031 - val_loss: 0.0606\n",
            "Epoch 150/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0029 - val_loss: 0.0577\n",
            "2/2 [==============================] - 1s 395ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n",
            "\u001b[32m[I 2022-11-17 17:56:50,784]\u001b[0m Trial 1 finished with value: 0.07257944318970125 and parameters: {'learning_rate': 0.0005877553634564128, 'head_size': 136, 'num_heads': 3, 'ff_dim': 2, 'num_transformer_bock': 4, 'decay': 1e-05, 'dropout_rate': 0.3, 'epochs': 150}. Best is trial 0 with value: 0.04525207126366099.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:686: UserWarning: The distribution is specified by [3, 4] and step=5, but the range is not divisible by `step`. It will be replaced by [3, 3].\n",
            "  low=low, old_high=old_high, high=high, step=step\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:686: UserWarning: The distribution is specified by [2, 3] and step=4, but the range is not divisible by `step`. It will be replaced by [2, 2].\n",
            "  low=low, old_high=old_high, high=high, step=step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 12s 973ms/step - loss: 1.7689 - val_loss: 0.2869\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.2699 - val_loss: 0.1319\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.1318 - val_loss: 0.0839\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0969 - val_loss: 0.0776\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0741 - val_loss: 0.0803\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0632 - val_loss: 0.1009\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0546 - val_loss: 0.0927\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0462 - val_loss: 0.1154\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0432 - val_loss: 0.1115\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0376 - val_loss: 0.1179\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0338 - val_loss: 0.1168\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0318 - val_loss: 0.1348\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0299 - val_loss: 0.1190\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0271 - val_loss: 0.1264\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0258 - val_loss: 0.1344\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0242 - val_loss: 0.1228\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0243 - val_loss: 0.1331\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 8s 928ms/step - loss: 0.0226 - val_loss: 0.1409\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0212 - val_loss: 0.1417\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0202 - val_loss: 0.1238\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0189 - val_loss: 0.1296\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0195 - val_loss: 0.1497\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.0183 - val_loss: 0.1304\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 8s 927ms/step - loss: 0.0180 - val_loss: 0.1324\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 8s 943ms/step - loss: 0.0159 - val_loss: 0.1405\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 8s 946ms/step - loss: 0.0148 - val_loss: 0.1283\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0161 - val_loss: 0.1434\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 8s 924ms/step - loss: 0.0155 - val_loss: 0.1396\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 8s 932ms/step - loss: 0.0149 - val_loss: 0.1318\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0152 - val_loss: 0.1352\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0147 - val_loss: 0.1362\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0153 - val_loss: 0.1261\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0140 - val_loss: 0.1361\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0144 - val_loss: 0.1306\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0133 - val_loss: 0.1205\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0120 - val_loss: 0.1420\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0122 - val_loss: 0.1142\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0111 - val_loss: 0.1154\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0111 - val_loss: 0.1184\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0111 - val_loss: 0.1207\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 0.0112 - val_loss: 0.1208\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0094 - val_loss: 0.0931\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0112 - val_loss: 0.1048\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0104 - val_loss: 0.1049\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0090 - val_loss: 0.0956\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0083 - val_loss: 0.0947\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0096 - val_loss: 0.1075\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 0.0088 - val_loss: 0.0888\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 0.0097 - val_loss: 0.0792\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0086 - val_loss: 0.0786\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0086 - val_loss: 0.0907\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0083 - val_loss: 0.0770\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0088 - val_loss: 0.0667\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0088 - val_loss: 0.0701\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0075 - val_loss: 0.0791\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0072 - val_loss: 0.0617\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0087 - val_loss: 0.0658\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0067 - val_loss: 0.0647\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0072 - val_loss: 0.0667\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0066 - val_loss: 0.0665\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 8s 925ms/step - loss: 0.0070 - val_loss: 0.0523\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 8s 934ms/step - loss: 0.0073 - val_loss: 0.0622\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 8s 945ms/step - loss: 0.0070 - val_loss: 0.0531\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 0.0070 - val_loss: 0.0606\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0056 - val_loss: 0.0583\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.0071 - val_loss: 0.0581\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 8s 929ms/step - loss: 0.0069 - val_loss: 0.0506\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0065 - val_loss: 0.0565\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0062 - val_loss: 0.0550\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0058 - val_loss: 0.0553\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0058 - val_loss: 0.0542\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0056 - val_loss: 0.0560\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0062 - val_loss: 0.0538\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 0.0060 - val_loss: 0.0564\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0049 - val_loss: 0.0545\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0062 - val_loss: 0.0576\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0056 - val_loss: 0.0538\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0049 - val_loss: 0.0523\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0046 - val_loss: 0.0553\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0055 - val_loss: 0.0584\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0049 - val_loss: 0.0527\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0049 - val_loss: 0.0525\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 0.0051 - val_loss: 0.0539\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0049 - val_loss: 0.0554\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0059 - val_loss: 0.0531\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0049 - val_loss: 0.0546\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0052 - val_loss: 0.0555\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0046 - val_loss: 0.0562\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0049 - val_loss: 0.0577\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 8s 914ms/step - loss: 0.0042 - val_loss: 0.0516\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0044 - val_loss: 0.0530\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0043 - val_loss: 0.0534\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 8s 933ms/step - loss: 0.0058 - val_loss: 0.0579\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 8s 941ms/step - loss: 0.0046 - val_loss: 0.0550\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 8s 934ms/step - loss: 0.0048 - val_loss: 0.0539\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 0.0051 - val_loss: 0.0540\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 8s 918ms/step - loss: 0.0048 - val_loss: 0.0544\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0059 - val_loss: 0.0541\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0041 - val_loss: 0.0551\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.0042 - val_loss: 0.0543\n",
            "2/2 [==============================] - 1s 344ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 43). These functions will not be directly callable after loading.\n",
            "\u001b[32m[I 2022-11-17 18:10:42,349]\u001b[0m Trial 2 finished with value: 0.0676102317504762 and parameters: {'learning_rate': 0.0007617095272576973, 'head_size': 177, 'num_heads': 3, 'ff_dim': 2, 'num_transformer_bock': 3, 'decay': 0.001, 'dropout_rate': 0.3, 'epochs': 100}. Best is trial 0 with value: 0.04525207126366099.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:686: UserWarning: The distribution is specified by [3, 4] and step=5, but the range is not divisible by `step`. It will be replaced by [3, 3].\n",
            "  low=low, old_high=old_high, high=high, step=step\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:686: UserWarning: The distribution is specified by [2, 3] and step=4, but the range is not divisible by `step`. It will be replaced by [2, 2].\n",
            "  low=low, old_high=old_high, high=high, step=step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 12s 940ms/step - loss: 2.3552 - val_loss: 1.5667\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.6618 - val_loss: 0.1820\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.2830 - val_loss: 0.2669\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.2305 - val_loss: 0.1426\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.1759 - val_loss: 0.1584\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.1415 - val_loss: 0.1370\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.1341 - val_loss: 0.1243\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 8s 877ms/step - loss: 0.1255 - val_loss: 0.1252\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.1078 - val_loss: 0.1082\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.1112 - val_loss: 0.1022\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 8s 910ms/step - loss: 0.0996 - val_loss: 0.1009\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.1042 - val_loss: 0.1003\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0948 - val_loss: 0.0997\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0923 - val_loss: 0.0975\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 0.0839 - val_loss: 0.0973\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0838 - val_loss: 0.0949\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0774 - val_loss: 0.0949\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0761 - val_loss: 0.0917\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0719 - val_loss: 0.0914\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0712 - val_loss: 0.0914\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0722 - val_loss: 0.0880\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0676 - val_loss: 0.0850\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0653 - val_loss: 0.0870\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0643 - val_loss: 0.0878\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0611 - val_loss: 0.0823\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0618 - val_loss: 0.0924\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 8s 881ms/step - loss: 0.0588 - val_loss: 0.0835\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0610 - val_loss: 0.0841\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0536 - val_loss: 0.0862\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0553 - val_loss: 0.0855\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0520 - val_loss: 0.0838\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 8s 907ms/step - loss: 0.0533 - val_loss: 0.0833\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0512 - val_loss: 0.0827\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0506 - val_loss: 0.0838\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0464 - val_loss: 0.0814\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 8s 901ms/step - loss: 0.0470 - val_loss: 0.0832\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 8s 893ms/step - loss: 0.0439 - val_loss: 0.0826\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 8s 949ms/step - loss: 0.0450 - val_loss: 0.0814\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 8s 899ms/step - loss: 0.0418 - val_loss: 0.0819\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0435 - val_loss: 0.0812\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0399 - val_loss: 0.0811\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 8s 940ms/step - loss: 0.0395 - val_loss: 0.0808\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 8s 921ms/step - loss: 0.0415 - val_loss: 0.0802\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 8s 904ms/step - loss: 0.0381 - val_loss: 0.0807\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0369 - val_loss: 0.0827\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 8s 919ms/step - loss: 0.0335 - val_loss: 0.0828\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 8s 930ms/step - loss: 0.0342 - val_loss: 0.0836\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 8s 920ms/step - loss: 0.0341 - val_loss: 0.0831\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0334 - val_loss: 0.0853\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 8s 900ms/step - loss: 0.0325 - val_loss: 0.0847\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0339 - val_loss: 0.0855\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0315 - val_loss: 0.0853\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0317 - val_loss: 0.0875\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 8s 894ms/step - loss: 0.0310 - val_loss: 0.0868\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 8s 889ms/step - loss: 0.0312 - val_loss: 0.0848\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 8s 882ms/step - loss: 0.0304 - val_loss: 0.0858\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0291 - val_loss: 0.0836\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0279 - val_loss: 0.0863\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 8s 894ms/step - loss: 0.0267 - val_loss: 0.0859\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0261 - val_loss: 0.0868\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0256 - val_loss: 0.0874\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0261 - val_loss: 0.0866\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0246 - val_loss: 0.0881\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0266 - val_loss: 0.0881\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0224 - val_loss: 0.0897\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0248 - val_loss: 0.0901\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 8s 896ms/step - loss: 0.0233 - val_loss: 0.0899\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 0.0237 - val_loss: 0.0912\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 8s 891ms/step - loss: 0.0226 - val_loss: 0.0899\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0222 - val_loss: 0.0935\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 8s 903ms/step - loss: 0.0219 - val_loss: 0.0919\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 8s 915ms/step - loss: 0.0217 - val_loss: 0.0956\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 8s 906ms/step - loss: 0.0203 - val_loss: 0.0932\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 8s 909ms/step - loss: 0.0199 - val_loss: 0.0935\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 8s 933ms/step - loss: 0.0189 - val_loss: 0.0931\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 9s 962ms/step - loss: 0.0197 - val_loss: 0.0944\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 8s 921ms/step - loss: 0.0183 - val_loss: 0.0918\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 8s 908ms/step - loss: 0.0188 - val_loss: 0.0939\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 8s 933ms/step - loss: 0.0182 - val_loss: 0.0923\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 8s 921ms/step - loss: 0.0192 - val_loss: 0.0933\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0178 - val_loss: 0.0933\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 8s 897ms/step - loss: 0.0167 - val_loss: 0.0957\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 8s 895ms/step - loss: 0.0160 - val_loss: 0.0959\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 8s 898ms/step - loss: 0.0163 - val_loss: 0.0945\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 8s 892ms/step - loss: 0.0187 - val_loss: 0.0948\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0159 - val_loss: 0.0941\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0164 - val_loss: 0.0954\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0168 - val_loss: 0.0984\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0148 - val_loss: 0.0957\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0149 - val_loss: 0.0945\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0141 - val_loss: 0.0949\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0148 - val_loss: 0.0962\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 8s 894ms/step - loss: 0.0174 - val_loss: 0.0964\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 8s 905ms/step - loss: 0.0147 - val_loss: 0.0977\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 8s 887ms/step - loss: 0.0148 - val_loss: 0.0970\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 8s 885ms/step - loss: 0.0137 - val_loss: 0.0957\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 8s 886ms/step - loss: 0.0134 - val_loss: 0.0943\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 8s 890ms/step - loss: 0.0155 - val_loss: 0.0933\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 8s 883ms/step - loss: 0.0123 - val_loss: 0.0927\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 8s 884ms/step - loss: 0.0119 - val_loss: 0.0945\n",
            "2/2 [==============================] - 1s 327ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 43). These functions will not be directly callable after loading.\n",
            "\u001b[32m[I 2022-11-17 18:24:17,370]\u001b[0m Trial 3 finished with value: 0.0935080403037996 and parameters: {'learning_rate': 0.00017615740756929225, 'head_size': 166, 'num_heads': 3, 'ff_dim': 2, 'num_transformer_bock': 3, 'decay': 1e-05, 'dropout_rate': 0.0, 'epochs': 100}. Best is trial 0 with value: 0.04525207126366099.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:686: UserWarning: The distribution is specified by [3, 4] and step=5, but the range is not divisible by `step`. It will be replaced by [3, 3].\n",
            "  low=low, old_high=old_high, high=high, step=step\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:686: UserWarning: The distribution is specified by [2, 3] and step=4, but the range is not divisible by `step`. It will be replaced by [2, 2].\n",
            "  low=low, old_high=old_high, high=high, step=step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 15s 1s/step - loss: 2.6471 - val_loss: 0.1473\n",
            "Epoch 2/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.1725 - val_loss: 0.1325\n",
            "Epoch 3/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.1284 - val_loss: 0.0799\n",
            "Epoch 4/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0901 - val_loss: 0.0745\n",
            "Epoch 5/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0724 - val_loss: 0.0872\n",
            "Epoch 6/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0535 - val_loss: 0.1017\n",
            "Epoch 7/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0466 - val_loss: 0.1198\n",
            "Epoch 8/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0403 - val_loss: 0.2279\n",
            "Epoch 9/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0368 - val_loss: 0.2457\n",
            "Epoch 10/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0330 - val_loss: 0.2308\n",
            "Epoch 11/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0316 - val_loss: 0.2297\n",
            "Epoch 12/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0288 - val_loss: 0.2131\n",
            "Epoch 13/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0271 - val_loss: 0.2837\n",
            "Epoch 14/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0261 - val_loss: 0.2692\n",
            "Epoch 15/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0230 - val_loss: 0.2108\n",
            "Epoch 16/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0234 - val_loss: 0.2478\n",
            "Epoch 17/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0228 - val_loss: 0.2443\n",
            "Epoch 18/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0193 - val_loss: 0.2510\n",
            "Epoch 19/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0188 - val_loss: 0.2460\n",
            "Epoch 20/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0192 - val_loss: 0.2836\n",
            "Epoch 21/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0169 - val_loss: 0.3179\n",
            "Epoch 22/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0192 - val_loss: 0.2927\n",
            "Epoch 23/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0178 - val_loss: 0.2178\n",
            "Epoch 24/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0168 - val_loss: 0.2340\n",
            "Epoch 25/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0158 - val_loss: 0.2294\n",
            "Epoch 26/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0155 - val_loss: 0.2457\n",
            "Epoch 27/150\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0150 - val_loss: 0.2434\n",
            "Epoch 28/150\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0149 - val_loss: 0.2780\n",
            "Epoch 29/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0142 - val_loss: 0.2223\n",
            "Epoch 30/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0144 - val_loss: 0.2074\n",
            "Epoch 31/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0158 - val_loss: 0.2071\n",
            "Epoch 32/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0137 - val_loss: 0.2037\n",
            "Epoch 33/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0118 - val_loss: 0.2022\n",
            "Epoch 34/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0132 - val_loss: 0.2248\n",
            "Epoch 35/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0121 - val_loss: 0.2076\n",
            "Epoch 36/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0117 - val_loss: 0.2034\n",
            "Epoch 37/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0128 - val_loss: 0.1987\n",
            "Epoch 38/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0117 - val_loss: 0.1729\n",
            "Epoch 39/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0127 - val_loss: 0.1099\n",
            "Epoch 40/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0140 - val_loss: 0.1074\n",
            "Epoch 41/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0137 - val_loss: 0.1234\n",
            "Epoch 42/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0112 - val_loss: 0.1209\n",
            "Epoch 43/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0092 - val_loss: 0.1277\n",
            "Epoch 44/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0093 - val_loss: 0.1385\n",
            "Epoch 45/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0100 - val_loss: 0.1217\n",
            "Epoch 46/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0101 - val_loss: 0.1385\n",
            "Epoch 47/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0096 - val_loss: 0.1268\n",
            "Epoch 48/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0119 - val_loss: 0.1066\n",
            "Epoch 49/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0099 - val_loss: 0.1175\n",
            "Epoch 50/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0086 - val_loss: 0.1165\n",
            "Epoch 51/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0112 - val_loss: 0.1182\n",
            "Epoch 52/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0089 - val_loss: 0.0963\n",
            "Epoch 53/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0084 - val_loss: 0.1000\n",
            "Epoch 54/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0085 - val_loss: 0.1018\n",
            "Epoch 55/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0088 - val_loss: 0.1009\n",
            "Epoch 56/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0077 - val_loss: 0.0971\n",
            "Epoch 57/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0103 - val_loss: 0.0909\n",
            "Epoch 58/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0079 - val_loss: 0.0926\n",
            "Epoch 59/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0066 - val_loss: 0.0903\n",
            "Epoch 60/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0079 - val_loss: 0.0914\n",
            "Epoch 61/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0069 - val_loss: 0.0885\n",
            "Epoch 62/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0075 - val_loss: 0.0891\n",
            "Epoch 63/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0072 - val_loss: 0.0885\n",
            "Epoch 64/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0076 - val_loss: 0.0877\n",
            "Epoch 65/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0066 - val_loss: 0.0885\n",
            "Epoch 66/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0075 - val_loss: 0.0869\n",
            "Epoch 67/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0067 - val_loss: 0.0843\n",
            "Epoch 68/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0068 - val_loss: 0.0845\n",
            "Epoch 69/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0059 - val_loss: 0.0914\n",
            "Epoch 70/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0062 - val_loss: 0.0818\n",
            "Epoch 71/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0053 - val_loss: 0.0817\n",
            "Epoch 72/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0060 - val_loss: 0.0818\n",
            "Epoch 73/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0075 - val_loss: 0.0801\n",
            "Epoch 74/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0066 - val_loss: 0.0817\n",
            "Epoch 75/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0059 - val_loss: 0.0852\n",
            "Epoch 76/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0062 - val_loss: 0.0806\n",
            "Epoch 77/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0065 - val_loss: 0.0865\n",
            "Epoch 78/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0057 - val_loss: 0.0783\n",
            "Epoch 79/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0056 - val_loss: 0.0808\n",
            "Epoch 80/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0072 - val_loss: 0.0783\n",
            "Epoch 81/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0053 - val_loss: 0.0830\n",
            "Epoch 82/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0049 - val_loss: 0.0802\n",
            "Epoch 83/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0057 - val_loss: 0.0785\n",
            "Epoch 84/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0060 - val_loss: 0.0746\n",
            "Epoch 85/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0052 - val_loss: 0.0756\n",
            "Epoch 86/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0057 - val_loss: 0.0713\n",
            "Epoch 87/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0053 - val_loss: 0.0760\n",
            "Epoch 88/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0053 - val_loss: 0.0803\n",
            "Epoch 89/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0055 - val_loss: 0.0809\n",
            "Epoch 90/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0051 - val_loss: 0.0802\n",
            "Epoch 91/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0046 - val_loss: 0.0720\n",
            "Epoch 92/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0049 - val_loss: 0.0764\n",
            "Epoch 93/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0065 - val_loss: 0.0753\n",
            "Epoch 94/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0052 - val_loss: 0.0722\n",
            "Epoch 95/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0048 - val_loss: 0.0772\n",
            "Epoch 96/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0043 - val_loss: 0.0738\n",
            "Epoch 97/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0045 - val_loss: 0.0743\n",
            "Epoch 98/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0052 - val_loss: 0.0736\n",
            "Epoch 99/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0044 - val_loss: 0.0739\n",
            "Epoch 100/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0044 - val_loss: 0.0739\n",
            "Epoch 101/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0043 - val_loss: 0.0710\n",
            "Epoch 102/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0041 - val_loss: 0.0730\n",
            "Epoch 103/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0043 - val_loss: 0.0740\n",
            "Epoch 104/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0059 - val_loss: 0.0725\n",
            "Epoch 105/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0046 - val_loss: 0.0758\n",
            "Epoch 106/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0049 - val_loss: 0.0723\n",
            "Epoch 107/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0044 - val_loss: 0.0726\n",
            "Epoch 108/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0049 - val_loss: 0.0743\n",
            "Epoch 109/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0045 - val_loss: 0.0743\n",
            "Epoch 110/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0061 - val_loss: 0.0739\n",
            "Epoch 111/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0044 - val_loss: 0.0762\n",
            "Epoch 112/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0043 - val_loss: 0.0753\n",
            "Epoch 113/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0048 - val_loss: 0.0732\n",
            "Epoch 114/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0041 - val_loss: 0.0758\n",
            "Epoch 115/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0038 - val_loss: 0.0758\n",
            "Epoch 116/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0051 - val_loss: 0.0764\n",
            "Epoch 117/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0047 - val_loss: 0.0777\n",
            "Epoch 118/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0040 - val_loss: 0.0739\n",
            "Epoch 119/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0031 - val_loss: 0.0732\n",
            "Epoch 120/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0036 - val_loss: 0.0742\n",
            "Epoch 121/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0038 - val_loss: 0.0732\n",
            "Epoch 122/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0037 - val_loss: 0.0751\n",
            "Epoch 123/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0039 - val_loss: 0.0698\n",
            "Epoch 124/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0041 - val_loss: 0.0691\n",
            "Epoch 125/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0033 - val_loss: 0.0729\n",
            "Epoch 126/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0043 - val_loss: 0.0745\n",
            "Epoch 127/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0041 - val_loss: 0.0779\n",
            "Epoch 128/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0047 - val_loss: 0.0727\n",
            "Epoch 129/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0036 - val_loss: 0.0730\n",
            "Epoch 130/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0035 - val_loss: 0.0718\n",
            "Epoch 131/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0035 - val_loss: 0.0720\n",
            "Epoch 132/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0040 - val_loss: 0.0700\n",
            "Epoch 133/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0041 - val_loss: 0.0724\n",
            "Epoch 134/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0036 - val_loss: 0.0711\n",
            "Epoch 135/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0033 - val_loss: 0.0684\n",
            "Epoch 136/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0048 - val_loss: 0.0705\n",
            "Epoch 137/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0040 - val_loss: 0.0720\n",
            "Epoch 138/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0052 - val_loss: 0.0749\n",
            "Epoch 139/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0037 - val_loss: 0.0719\n",
            "Epoch 140/150\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0040 - val_loss: 0.0719\n",
            "Epoch 141/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0047 - val_loss: 0.0783\n",
            "Epoch 142/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0041 - val_loss: 0.0757\n",
            "Epoch 143/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0036 - val_loss: 0.0741\n",
            "Epoch 144/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0037 - val_loss: 0.0711\n",
            "Epoch 145/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0038 - val_loss: 0.0724\n",
            "Epoch 146/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0038 - val_loss: 0.0735\n",
            "Epoch 147/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0029 - val_loss: 0.0753\n",
            "Epoch 148/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0031 - val_loss: 0.0719\n",
            "Epoch 149/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0038 - val_loss: 0.0719\n",
            "Epoch 150/150\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0034 - val_loss: 0.0699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3e9adf29e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 412ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 57). These functions will not be directly callable after loading.\n",
            "\u001b[32m[I 2022-11-17 18:50:59,487]\u001b[0m Trial 4 finished with value: 0.0756970240611727 and parameters: {'learning_rate': 0.0008507997862573048, 'head_size': 162, 'num_heads': 3, 'ff_dim': 2, 'num_transformer_bock': 4, 'decay': 0.001, 'dropout_rate': 0.3, 'epochs': 150}. Best is trial 0 with value: 0.04525207126366099.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial = study.best_trial\n",
        "print('MSE: {}'.format(trial.value))\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))\n",
        "with open(\"/content/drive/MyDrive/tcn+transformer/{}.pickle\".format(study.best_trial.number), \"rb\") as fin:\n",
        "    best_tcn = pickle.load(fin)\n",
        "model = best_tcn\n",
        "y_pred = model.predict(input_test)\n",
        "mse = mean_squared_error(output_test, y_pred)\n",
        "print(mse)\n",
        "print(study.best_trial.number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F55Z5RFAcDZB",
        "outputId": "927c1a84-0a47-4fe8-ce1f-c781776ba163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.04525207126366099\n",
            "Best hyperparameters: {'learning_rate': 5.8237632001036017e-05, 'head_size': 165, 'num_heads': 3, 'ff_dim': 2, 'num_transformer_bock': 4, 'decay': 1e-05, 'dropout_rate': 0.3, 'epochs': 100}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3e9a9ec680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 438ms/step\n",
            "0.04525207126366099\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/tcn+transformer/{}.pickle\".format(0), \"rb\") as fin:\n",
        "    best_transf = pickle.load(fin)\n",
        "model_transf = best_transf\n",
        "y_pred = model_transf.predict(input_test)\n",
        "test_nrmse = np.sqrt(np.sum((output_test - y_pred)**2))/np.sqrt(np.sum((output_test - np.average(output_test))**2))\n",
        "mse = mean_squared_error(output_test, y_pred)\n",
        "print(\"test_nrmse is: \",test_nrmse)\n",
        "print(\"test_mse is:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMLNRqYG4GAE",
        "outputId": "125b20c9-021f-4cce-b3b6-d3528dee9412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 2s 461ms/step\n",
            "test_nrmse is:  0.8409424951264388\n",
            "test_mse is: 0.04525207126366099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(output_test[:, 0], y_pred[:, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "i_5VXNEA4koI",
        "outputId": "097cc944-e0c9-4375-e6bd-955beeb04b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f3e99579990>"
            ]
          },
          "metadata": {},
          "execution_count": 406
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}